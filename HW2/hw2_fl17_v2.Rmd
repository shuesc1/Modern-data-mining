---
title: "Modern Data Mining - HW 2"
author:
- Antina Lee
- Maria Diaz Ortiz
- Joseph Haymaker
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=5, fig.width=11, warning = F)

# constants for homework assignments
hw_num <- 2
hw_due_date <- "10 October, 2017"
```
<!---
## Overview / Instructions

This is homework #`r paste(hw_num)` of STAT 471/571/701. It will be **due on `r paste(hw_due_date)` by 11:59 PM** on Canvas. You can directly edit this file to add your answers. Submit the Rmd file, a PDF or word or HTML version with only 1 submission per HW team.


Solutions will be posted. Make sure to go through these files to pick up some tips.
--->

```{r library, include=FALSE}
# add your library imports here:
#install.packages("maps")
library(maps)
library(dplyr)
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
library(ggplot2)
library(dplyr)
```

## Problem 0

__Review the code and concepts covered during lecture: model selection and penalized regression through elastic net. __

## Problem 1: Model Selection

__Do ISLR, page 262, problem 8, and write up the answer here. This question is designed to help understanding of model selection through simulations. __

(a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector ε of length n = 100.

We 'll first set seed to keep our numbers consistent and prepare the x and noise variables.
```{r}
set.seed(1)
x <- rnorm(100)
noise <- rnorm(100) 
```

(b) Generate a response vector Y of length n = 100 according to the model
Y = β0 +β1X +β2X2 +β3X3 +ε, where β0, β1, β2, and β3 are constants of your choice.
```{r}
b0 <- 5
b1 <- 4
b2 <- 3
b3 <- 2
y <- b0 + b1*x + b2*x^2 + b3*x^3 + noise
```

(c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X,X2,...,X10. What is the best model obtained according to Cp, BIC, and adjusted R2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y .

We first create a dataframe to contain x and y.
```{r}
data1 <- data.frame(y = y,x = x)
```

We have loaded the leaps library at the top and can use it to run subset selection with regsubsets and store the summary as f.e.
```{r}
fit.exh <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data1, nvmax = 10)
f.e <- summary(fit.exh)
```

We plot the summary to compare different criteria. 
```{r}
par(mfrow = c(3,1))
plot(f.e$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(f.e$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(f.e$adjr2, xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```
In the cp and bic plots, we look for the number of variables with smallers or nearlly smallest $C_p$'s and $BIC$'s. The plots show that 3-variables is optimal according to the $C_p$ and $BIC$ criteria.
In the adjusted r plot, we look for the number of variables that maximizes or nearly maximizees $adjr2$. The plot shows that we achieve this at 3 variables. Therefore, we conclude that we can use 3-variable models.

With 3 variables, we have the follow coefficients:
```{r}
coef(fit.exh,3)
```
Our final model of 3 variables (x, x^2, x^3) are as follows.
All variables have coefficients different from 0 at the 0.05 significance level.
```{r}
fit.final <- lm(y ~ x + I(x^2) + I(x^3), data1)
summary(fit.final)
```

(d) Repeat (c), using forward stepwise selection and also using back- wards stepwise selection. How does your answer compare to the results in (c)?

We will first run regsubsets with forward stepwise selection.
```{r}
fit.forward <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data1, nvmax = 10, method = "forward")
f.f <- summary(fit.forward)
```
We plot the summary to compare different criteria. 
```{r}
par(mfrow = c(3,1))
plot(f.f$cp, main="Forward Selection", xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(f.f$bic, main="Forward Selection", xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(f.f$adjr2, main="Forward Selection", xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```

In the cp and bic plots, we look for the number of variables with smallers or nearlly smallest $C_p$'s and $BIC$'s. The plots show that 3-variables is optimal according to the $C_p$ and $BIC$ criteria.
In the adjusted r plot, we look for the number of variables that maximizes or nearly maximizees $adjr2$. The plot shows that we achieve this at 3 variables. Therefore, we conclude that we can use 3-variable models.

With 3 variables, we have the follow coefficients. 
```{r}
coef(fit.forward,3)
```
Our final model of 3 variables (x, x^2, x^3) are as follows: 
All variables have coefficients different from 0 at the 0.05 significance level.
```{r}
fit.finalforward <- lm(y ~ x + I(x^2) + I(x^3), data1)
summary(fit.finalforward)
```

We will now run regsubsets with backward stepwise selection.
```{r}
fit.backward <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data1, nvmax = 10, method = "backward")
f.b <- summary(fit.backward)
```
We plot the summary to compare different criteria. 
```{r}
par(mfrow = c(3,1))
plot(f.b$cp, main="Backward Selection", xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(f.b$bic, main="Backward Selection", xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(f.b$adjr2, main="Backward Selection", xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```

In the cp and bic plots, we look for the number of variables with smallers or nearlly smallest $C_p$'s and $BIC$'s. The plots show that 3-variables is optimal according to the $C_p$ and $BIC$ criteria.
In the adjusted r plot, we look for the number of variables that maximizes or nearly maximizees $adjr2$. The plot shows that we achieve this at 3 variables. Therefore, we conclude that we can use 3-variable models.

With 3 variables, we have the follow coefficients. 
```{r}
coef(fit.backward,3)
```
Our final model of 3 variables (x, x^2, x^3) are as follows: 
All variables have coefficients different from 0 at the 0.05 significance level.
```{r}
fit.finalbackward <- lm(y ~ x + I(x^2) + I(x^3), data1)
summary(fit.finalbackward)
```


(e) Now fit a lasso model to the simulated data, again using X,X2, . . . , X 10 as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss the results obtained.

We first prepare the input x matrix for glmnet.
```{r}
X <- model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data1)[, -1]
```
We can run cv.glmnet with X and y and plot the results.
```{r}
fit.cv <- cv.glmnet(X, y, alpha = 1, nfolds = 10) 
plot(fit.cv)
```

Using lamdba.min as the optimal value of λ, we find it to be 0.07087723. This corresponds to 6 non-zero betas
```{r}
minlambda <- fit.cv$lambda.min
minlambda
```

The 6 non-zero betas are "x", "I(x^2)", "I(x^3)", "I(x^4)", "I(x^5)", "I(x^7)" with coefficients shown below:
```{r}
coef.min <- coef(fit.cv, s="lambda.min")  
coef.min <- coef.min[which(coef.min !=0),]   # get the non=zero coefficients
coef.min  # set of predictors chosen and their coefficients
rownames(as.matrix(coef.min))
```

Now that we know which coefficients to use, we prepare a lm formulae for the fit model.
```{r}
var.min <- rownames(as.matrix(coef.min)) # output the names of non-zero coefficients
lm.input <- as.formula(paste("y", "~", paste(var.min[-1], collapse = "+"))) # prepare for lm fomulae
lm.input
```

We can then fit the linear model with LASSO output variables.
```{r}
fit.min.lm <- lm(lm.input, data=data1)
lm.output <- coef(fit.min.lm) # output lm estimates
summary(fit.min.lm)
```

In summary, through Lasso estimation, we find an optimal λ (lambda.min) that corresponds to 6 non-zero coefficients. After fitting a linear model with these 6 coefficients, our final model has R-squared of 0.9909 with 3 of the 6 coefficients being significant at the 0.05 level.

(f) Now generate a response vector Y according to the model Y = β0 + β7X7 + ε,
and perform best subset selection and the lasso. Discuss the results obtained.

We'll first perform best subset selection.
```{r}
b7 <- 3
y2 <- b0 + b7*x^7 + noise
```

(c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X,X2,...,X10. What is the best model obtained according to Cp, BIC, and adjusted R2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y .

We first create a dataframe to contain x and y. Since we have loaded leaps library above, we won't load it again.
```{r}
data2 <- data.frame(y=y2, x=x)
```

We can then run subset selection with regsubsets and store the summary as f.e.
```{r}
fit.exh2 <- regsubsets(y2 ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data=data1, nvmax=10)
f.e2 <- summary(fit.exh2)
```

We plot the summary to compare different criteria. 
```{r}
par(mfrow = c(3, 1))
plot(f.e2$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(f.e2$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(f.e2$adjr2, xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)
```
The CP critierion is minimized at 2 variables.
The BIC critierion is minimized at 1 variable.
The adjr2 critierion is nearly maximized at 3 variables. 

With 2 variables ($C_p$ model), the variables used are x^2 and x^7 with cofficients as follows:
```{r}
coef(fit.exh2,2)
```
With 1 variable ($BIS$ model), the variable used is x^7 with cofficient as follows:
```{r}
coef(fit.exh2,1)
```
With 3 variables ($adjr2$ model), the variables used are x^2, x^5, x^7 with cofficients as follows:
```{r}
coef(fit.exh2,3)
```

When we run fit linear models for each of the models and study their summaries.
```{r}
fit2.cp <- lm(y2 ~ I(x^2) + I(x^7), data2)
fit2.bic <- lm(y2 ~ I(x^7), data2)
fit2.adjr2 <- lm(y2 ~ I(x^2) +  I(x^5) + I(x^7) , data2)
summary(fit2.cp)
summary(fit2.bic)
summary(fit2.adjr2)
```

Through the lm summaries, we see that both the $C_p$ and $adjr2$ models have insignificant variables at the 0.05 level. They also returns similar R-squared values as the $BIC$ model. However, the $BIC$ model outshines them in that it does not have any insignificant varaibles. Given this, I would feel comfortable selecting the $BIC$ model of 1 variable as the best model for this data.

We will now fit the lasso model. 

We first prepare the input x matrix for glmnet.
```{r}
X3 <- model.matrix(y2 ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data=data2)[, -1]
```
We can run cv.glmnet with X and y and plot the results.
```{r}
fit.cv2 <- cv.glmnet(X3, y2, alpha=1, nfolds=10) 
plot(fit.cv2)
```

Using lamdba.min as the optimal value of λ, we find it to be 5.818618 and log(lambda.min) of 1.761063. This corresponds to 1 non-zero beta.
```{r}
minlambda2 <- fit.cv2$lambda.min
minlambda2
```

The non-zero beta is "I(x^7)". The coefficients are shown below:
```{r}
coef.min2 <- coef(fit.cv2, s="lambda.min")  
coef.min2 <- coef.min2[which(coef.min2 !=0),]   # get the non=zero coefficients
coef.min2  # set of predictors chosen and their coefficients
```

Now that we know which coefficient to use, we prepare a lm formulae for the fit model.
```{r}
var.min2 <- rownames(as.matrix(coef.min2)) # output the names of non-zero coefficients
lm.input2 <- as.formula(paste("y", "~", paste(var.min2[-1], collapse = "+"))) # prepare for lm fomulae
lm.input2
```

We can then fit the linear model with LASSO output variables.
```{r}
fit.min.lm2 <- lm(lm.input2, data=data2)
lm.output2 <- coef(fit.min.lm2) # output lm estimates
summary(fit.min.lm2)
```

In summary, through Lasso estimation, we find an optimal λ (lambda.min) that corresponds to 1 non-zero coefficients (x^7). After fitting a linear model with this coefficient, our final model has R-squared of 1 with the x^7 coefficient being significant at the 0.05 level.

## Problem 2: Regularization

Crime data continuation:  We use a subset of the crime data discussed in class, but only look at Florida and California. `crimedata` is available on Canvas; we show the code to clean here. 

```{r}
crime <- read.csv("CrimeData.csv", stringsAsFactors = F, na.strings = c("?"))
crime <- dplyr::filter(crime, state %in% c("FL", "CA"))
```

Our goal is to find the factors which relate to violent crime. This variable is included in crime as `crime$violentcrimes.perpop`.

**A)** __EDA__

* Clean the data first

#### Exploration & data cleaning:
``` {r,results='hide'}
#names(crime)
dim(crime) #369 communities x 147 variables
sum(is.na(crime)) #7204 NA entries
#dim(na.omit(crime)) #would leave 0 communities

# =============================data1 subset==============================
data1<- crime[,c(2,6:103,121,122,123,130:147)] #subset removing sparse police dept info
#names(data1)
names(data1[, 103:120]) #crime variables
sapply(data1, function(x) any(is.na(x))) # T/F for each var
apply(data1, 2, function(x) any(is.na(x)))  # column-wise missing
apply(data1, 1, function(x) any(is.na(x)))  # row-wise missing
sum(is.na(data1$violentcrimes.perpop)) #1 NA entry
var_names_out <- c("num.urban","other.percap", "num.underpov",
                   "num.vacant.house","num.murders","num.rapes",
                   "num.robberies", "num.assaults", "num.burglaries",
                   "num.larcenies", "num.autothefts", "num.arsons")
data1 <- data1[!(names(data1) %in% var_names_out)]
names_other_crimes <- c( "murder.perpop", "rapes.perpop",                   
                        "robberies.perpop",  "assaults.perpop",                
                        "burglaries.perpop", "larcenies.perpop",               
                        "autothefts.perpop", "arsons.perpop",                  
                         "nonviolentcrimes.perpop")

# =============================data2 subset==============================
data2 <- data1[!(names(data1) %in% names_other_crimes)] #removing all other crime data
dim(data2) #369 communities x 99 variables

# =============================data3 subset==============================
sum(is.na(data2)) #1 NA to remove
data3 <- na.omit(data2)
dim(data3) #368 communities x 99 variables
#write.csv(data3, "CrimeData_CA_FL_clean.csv", row.names = FALSE) #optional file creation of cleaned FL & CA data
names(data3)
```

#### Histogram 
```{r}
ggplot(data1, aes(x = violentcrimes.perpop)) +
  geom_histogram(binwidth = 40) +
  ggtitle("Histogram of violent crimes per population") +
  ylab("Frequency") #peaks between 250 & 750
#rownames(data1)[is.na(data1$violentcrimes.perpop)]
```

* Prepare a set of sensible factors/variables that you may use to build a model
* Show the heatmap with mean violent crime by state. You may also show a couple of your favorate summary statistics by state through the heatmaps.  

### Heatmaps 

\* (code chunks in .rmd file)

#### Violent Crime

```{r, echo=F}
# ====================Map of mean violent crime===============================
#crime.data <- read.csv("CrimeData.csv", header=T, na.string=c("", "?")) #load all states data
crime.data <- data3
data.s <- crime.data %>%
  group_by(state) %>%
  summarise(
    mean.violentcrime=mean(violentcrimes.perpop), 
    violentcrime.min=min(violentcrimes.perpop),
    violentcrime.max=max(violentcrimes.perpop),
    crime.rate=mean(violentcrimes.perpop, na.rm=TRUE), #ignore the missing values
    n=n())
violentcrime <- data.s[, c("state", "mean.violentcrime")]
#names(violentcrime)
violentcrime
violentcrime$region <- tolower(state.name[match(violentcrime$state, state.abb)])
violentcrime$center_lat  <- state.center$x[match(violentcrime$state, state.abb)]
violentcrime$center_long <- state.center$y[match(violentcrime$state, state.abb)]
states <- map_data("state") 
map <- merge(states, violentcrime, sort=FALSE, by="region", all.x=TRUE)
map <- map[order(map$order),]
ggplot(map, aes(x=long, y=lat, group=group))+
  geom_polygon(aes(fill=mean.violentcrime))+
  geom_path()+ 
  geom_text(data=violentcrime, aes(x=center_lat, y=center_long, group=NA, 
                             label=state, size=2), show.legend =FALSE)+
  scale_fill_continuous(limits=c(500, 1200),name="Mean violent crime",
                        low="light blue", high="dark blue")+
                        xlab("Map of mean violent crime")
```

#### Percent Unemployment

```{r, echo=F}
# ====================Map of percent unemployment===============================
#crime.data <- read.csv("CrimeData.csv", header=T, na.string=c("", "?")) #load all states data
crime.data <- data3
#names(crime.data)
data.s <- crime.data %>%
  group_by(state) %>%
  summarise(
    mean.unemployed=mean(pct.unemployed), 
    unemployed.min=min(pct.unemployed),
    unemployed.max=max(pct.unemployed),
    crime.rate=mean(violentcrimes.perpop, na.rm=TRUE), #ignore the missing values
    n=n())
unemployed <- data.s[, c("state", "mean.unemployed")]
#names(unemployed)
unemployed
unemployed$region <- tolower(state.name[match(unemployed$state, state.abb)])
unemployed$center_lat  <- state.center$x[match(unemployed$state, state.abb)]
unemployed$center_long <- state.center$y[match(unemployed$state, state.abb)]
states <- map_data("state") 
map <- merge(states, unemployed, sort=FALSE, by="region", all.x=TRUE)
map <- map[order(map$order),]
ggplot(map, aes(x=long, y=lat, group=group))+
  geom_polygon(aes(fill=mean.unemployed))+
  geom_path()+ 
  geom_text(data=unemployed, aes(x=center_lat, y=center_long, group=NA, 
                             label=state, size=2), show.legend =FALSE)+
  scale_fill_continuous(limits=c(5, 7),name="Percent unemployed",
                        low="light blue", high="dark blue")+
                        xlab("Map of mean percent unemployment per state")
```

#### Percent of population with children and never married

```{r, echo=F}
# ====================Map of percent of population with children and never married===============================
#crime.data <- read.csv("CrimeData.csv", header=T, na.string=c("", "?")) #load all states data
crime.data <- data3
#names(crime.data)
data.s <- crime.data %>%
  group_by(state) %>%
  summarise(
    mean.kidsnvrmarried=mean(pct.kids.nvrmarried), 
    kidsnvrmarried.min=min(pct.kids.nvrmarried),
    kidsnvrmarried.max=max(pct.kids.nvrmarried),
    crime.rate=mean(violentcrimes.perpop, na.rm=TRUE), #ignore the missing values
    n=n())
kidsnvrmarried <- data.s[, c("state", "mean.kidsnvrmarried")]
#names(kidsnvrmarried)
kidsnvrmarried
kidsnvrmarried$region <- tolower(state.name[match(kidsnvrmarried$state, state.abb)])
kidsnvrmarried$center_lat  <- state.center$x[match(kidsnvrmarried$state, state.abb)]
kidsnvrmarried$center_long <- state.center$y[match(kidsnvrmarried$state, state.abb)]
states <- map_data("state") 
map <- merge(states, kidsnvrmarried, sort=FALSE, by="region", all.x=TRUE)
map <- map[order(map$order),]
ggplot(map, aes(x=long, y=lat, group=group))+
  geom_polygon(aes(fill=mean.kidsnvrmarried))+
  geom_path()+ 
  geom_text(data=kidsnvrmarried, aes(x=center_lat, y=center_long, group=NA, 
                             label=state, size=2), show.legend =FALSE)+
  scale_fill_continuous(limits=c(3, 4),name="Percent of pop. w/ children and never married",
                        low="light blue", high="dark blue")+
                        xlab("Map of percent of population with children and never married")
```

#### Summary based on EDA

* After performing exploratory data analysis we find that the majority of the violent crime incidents in FL and CA communities fall between 250 and 750, and reach up to 3900. We then examined three heat maps for various characteristics, namely mean violent crime, mean unemployment, and mean percent of pop. with children and never married. We find that FL has roughly 300 more violent crimes. This contrasts with mean unemployment, where CA has a 1% lead on unemployment over FL. Mean percent of population with children and never married is only slightly higher in FL compared to CA, with a ~.6% difference between them.

**B)** __Use LASSO to choose a reasonable, small model. Fit an OLS model with the variables obtained. The final model should only include variables with p-values < 0.05. Note: you may choose to use lambda 1se or lambda min to answer the following questions where apply.__

#### Using a given lambda

``` {r,results='hide'}
#data3
#data3[, 99]
#data3$violentcrimes.perpop
Y <- data3$violentcrimes.perpop # extract Y
X <- model.matrix(violentcrimes.perpop~., data=data3)[, -1]
#colnames(X)

#============using given λ==================
fit.lambda <- glmnet(X,Y,alpha=1,lambda = 100)
names(fit.lambda)  
fit.lambda$lambda # lambda used
fit.lambda$beta
fit.lambda$df    # number of non-zero coeff's -- 3 features
fit.lambda$a0 # beta_0 -- 1836.963 
coef(fit.lambda) # beta hat of predictors in the original scales, same as fit.lambda$beta

# more advanced way to extract non-zero output
tmp_coeffs <- coef(fit.lambda)  # output the LASSO estimates
tmp_coeffs 
```

```{r}
data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x) # list non-zero variables
```

#### Using a set of lambda values

```{r results='hide'}
#=========================using a set of λ========================
attach(data3)
# names(data3)
Y <- data3$violentcrimes.perpop # extract Y
Y

# rows.names <- names(data3)
# lm.input <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) 
# lm.input

X <- model.matrix(violentcrimes.perpop~., data=data3)[, -99]
X
colnames(X)
fit.lambda <- glmnet(X, Y, alpha=1)
str(fit.lambda)
fit.lambda$lambda
plot(fit.lambda)
#<<<<<<CV to select λ>>>>>>>
fit.cv <- cv.glmnet(X, Y, alpha=1, nfolds=10 ) 
names(fit.cv)
fit.cv$cvm               # the mean cv error for each lambda
fit.cv$lambda.min #28.51188
fit.cv$nzero             # number of non-zero coeff's returned for each lambda
#plot(fit.cv$lambda, fit.cv$nzero, xlab="lambda", ylab="number of non-zeros")
plot(fit.cv$lambda , main  = "There are 100 lambdas used" , xlab = "Lambda Index" , ylab = "Lambda Value" ) 
head(data.frame( Cross.Validation.Erorr = fit.cv$cvm , Lambda = fit.cv$lambda))            
plot(fit.cv$lambda, fit.cv$cvm, xlab=expression(lambda), ylab="mean cv errors")
plot(fit.cv)

# using λ=lambda.min
# coef.min <- coef(fit.cv, s="lambda.min")  #s=c("lambda.1se","lambda.min") or lambda value
# coef.min <- coef.min[which(coef.min !=0),]   # get the non=zero coefficients
# coef.min  # the set of predictors chosen
# rownames(as.matrix(coef.min)) # shows only names, not estimates -- 15 variables

# using λ=lambda.1se
 coef.1se <- coef(fit.cv, s="lambda.1se")  
 coef.1se <- coef.1se[which(coef.1se !=0),] 
 coef.1se
 rownames(as.matrix(coef.1se)) # only 3 variables

# using all non-zero coefficients
# coef.nzero <-coef(fit.cv, nzero = 3) 
# coef.nzero <- coef.nzero[which(coef.nzero !=0), ]
# rownames(as.matrix(coef.nzero)) 

# final-- using lambda.min
coef.min <- coef(fit.cv, s="lambda.min")  #s=c("lambda.1se","lambda.min") or lambda value
coef.min <- coef.min[which(coef.min !=0),]   # get the non=zero coefficients
coef.min 
rownames(as.matrix(coef.min))
#using LASSO variables to fit an lm() model
var.min <- rownames(as.matrix(coef.min)) 
lm.input <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-99], collapse = "+"))) 
lm.input

fit.min.lm <- lm(lm.input, data=data3)
lm.output <- coef(fit.min.lm) # output lm estimates
summary(fit.min.lm) 
comp <- data.frame(coef.min, lm.output)
names(comp) <-c("estimates from LASSO", "lm estimates")
comp

```

__1. What is the model reported by LASSO?__

##### Using lambda.min:

```{r}
coef.min <- coef(fit.cv, s="lambda.min")  
coef.min <- coef.min[which(coef.min !=0),]   
var.min <- rownames(as.matrix(coef.min)) 
lm.input.lambdamin <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) 
lm.input.lambdamin
#summary(lm(lm.input.lambdamin, data3))
```

##### Using lambda.1se:

```{r}
coef.1se <- coef(fit.cv, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] 
#coef.1se
#rownames(as.matrix(coef.1se))
var.min <- rownames(as.matrix(coef.1se))
lm.input.lambda1se <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) 
lm.input.lambda1se
#summary(lm(lm.input.lambda1se, data3))
```


__2. What is the model after running OLS?__

##### Model after running OLS using lambda.min:

```{r}
comp
summary(lm(lm.input.lambdamin, data3))
```

<!--- this chunk not included, but was the model I originally got --->
```{r, eval=FALSE, include=FALSE}
summary(lm(violentcrimes.perpop ~ race.pctblack + pct.farmself.inc + pct.inv.inc + 
    asian.percap + male.pct.divorce + pct.kids2parents + pct.workmom + 
    num.kids.nvrmarried + pct.kids.nvrmarried + pct.english.only + 
    pct.house.occup + pct.house.vacant + med.yr.house.built + 
    pct.house.nophone + num.in.shelters, data3))
```

__3. What is your final model, after excluding high p-value variables? You will need to use model selection method to obtain this final model. Make it clear what criterion/criteria you have used and justify why they are appropriate.__

#### All subsets selection

```{r eval=FALSE}
#==============Obtaining final model with model selection===================
#<<<<<<All subsets>>>>>>>
fit.exh <- regsubsets(lm.input, data3, nvmax=6, method="exhaustive")
names(fit.exh)
summary(fit.exh)
exh.summary <- summary(fit.exh)
names(exh.summary)
exh.summary$rsq
par(mfrow=c(2,2))
plot(exh.summary$rss,xlab="Number of Variables", ylab="RSS",type="l")
plot(exh.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq",type="l")
which.max(exh.summary$adjr2)
points(which.max(exh.summary$adjr2),exh.summary$adjr2[which.max(exh.summary$adjr2)], col="red",cex=2,pch=20) #5 variables
plot(exh.summary$cp,xlab="number of variables",ylab="Cp", type = 'l')
which.min(exh.summary$cp)
points(which.min(exh.summary$cp),exh.summary$cp[which.min(exh.summary$cp)],col="red",cex=2,pch=20) # 5 variables
plot(exh.summary$bic,xlab="number of variables",ylab="BIC", type = 'l')
points(which.min(exh.summary$bic),exh.summary$bic[which.min(exh.summary$bic)],col="red",cex=2,pch=20) # 4 variables
# let's go with 5 variables
coef.exh <- coef(fit.exh,5)
var.min <- rownames(as.matrix(coef.exh)) # output the names
print("============5 variable model using all subsets==========")
lm.input2 <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) 
lm.input2
coef.exh
print("============BIC & Cp with 5 variables==========")
exh.summary$bic[5] # -397.9771
exh.summary$cp[5] # 6
```

#### Forward stepwise selection

```{r eval=FALSE}
#<<<<<<Forward stepwise selection>>>>>>>
fit.fwd=regsubsets(lm.input,data=data3 ,nvmax =5, method ="forward")
summary(fit.fwd)
fit.fwd.summary <-summary(fit.fwd)
names(fit.fwd.summary)
# criteria plots
par(mfrow=c(2,2))
plot(fit.fwd.summary$rss,xlab="Number of Variables", ylab="RSS",type="l")
plot(fit.fwd.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq",type="l")
which.max(fit.fwd.summary$adjr2)
points(5,fit.fwd.summary$adjr2[5], col="red",cex=2,pch=20) # 5 variables
plot(fit.fwd.summary$cp,xlab="number of variables",ylab="Cp", type = 'l')
points(which.min(fit.fwd.summary$cp),fit.fwd.summary$cp[which.min(fit.fwd.summary$cp)],col="red",cex=2,pch=20) # 5 variables
plot(fit.fwd.summary$bic,xlab="number of variables",ylab="BIC", type = 'l')
points(which.min(fit.fwd.summary$bic),fit.fwd.summary$bic[which.min(fit.fwd.summary$bic)],col="red",cex=2,pch=20) # 4 variables
# choose to go with 4 variables
coef.fit.fwd <- coef(fit.fwd,4)
var.min <- rownames(as.matrix(coef.fit.fwd)) # output the names
print("============4 variable model using forwards stepwise selection==========")
lm.input3 <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) # prepare for lm fomulae
lm.input3
coef.fit.fwd
print("============BIC & Cp with 4 variables==========")
fit.fwd.summary$bic[4] #-398.7349
fit.fwd.summary$cp[4] #9.10198
```


#### Backward stepwise selection
```{r eval=FALSE}
#<<<<<<Backward stepwise selection>>>>>>>
fit.bwd=regsubsets(lm.input,data=data3 ,nvmax =5,method ="backward")
summary(fit.bwd)
fit.bwd.summary <-summary(fit.bwd)
names(fit.bwd.summary)
# criteria plots
par(mfrow=c(2,2))
plot(fit.bwd.summary$rss,xlab="Number of Variables", ylab="RSS",type="l")
plot(fit.bwd.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq",type="l")
points(which.max(fit.bwd.summary$adjr2),fit.bwd.summary$adjr2[which.max(fit.bwd.summary$adjr2)], col="red",cex=2,pch=20) # 5 variables
plot(fit.bwd.summary$cp,xlab="number of variables",ylab="Cp", type = 'l')
points(which.min(fit.bwd.summary$cp),fit.bwd.summary$cp[which.min(fit.bwd.summary$cp)],col="red",cex=2,pch=20) # 5 variables
plot(fit.bwd.summary$bic,xlab="number of variables",ylab="BIC", type = 'l')
points(which.min(fit.bwd.summary$bic),fit.bwd.summary$bic[which.min(fit.bwd.summary$bic)],col="red",cex=2,pch=20) # 4 variables
# choose to go with 4 variables
coef.fit.bwd <- coef(fit.bwd,5)
var.min <- rownames(as.matrix(coef.fit.bwd)) # output the names
print("============5 variable model using backwards stepwise==========")
lm.input4 <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) 
lm.input4
coef.fit.bwd
print("============BIC with 5 variables==========")
fit.bwd.summary$bic[5] #-397.9771
fit.bwd.summary$cp[5] #6
```

#### Final specified model obtained by backward stepwise

_Our final specified model obtained by backward stepwise selection is:_

violentcrimes.perpop ~ race.pctblack + pct.kids2parents + pct.kids.nvrmarried + 
    pct.house.vacant + num.in.shelters

```{r}
summary(lm(violentcrimes.perpop ~ race.pctblack + pct.kids2parents + pct.kids.nvrmarried + 
    pct.house.vacant + num.in.shelters, data3))
```

<!--- not included but original results --->
```{r, eval=FALSE, include=FALSE}
summary(lm(violentcrimes.perpop ~ race.pctblack + asian.percap + male.pct.divorce + 
    pct.kids2parents + pct.workmom + pct.kids.nvrmarried + pct.english.only + 
    pct.house.occup + num.in.shelters, data3))
```

We took the `lm()` inputs obtained from LASSO and then performed model selection by comparing the best models from all subset, forward stepwise, and backward stepwise selection techniques. We reached the conclusion that the above model was the best based on lowest Cp score of the all subset, forward stepwise, and backward stepwise selection models.


**C)** __Now, instead of Lasso, we want to consider how changing the value of alpha (i.e. mixing between Lasso and Ridge) will affect the model. Cross-validate between alpha and lambda, instead of just lambda. Note that the final model may have variables with p-values higher than 0.05; this is because we are optimizing for accuracy rather than parsimoniousness.__

__1. What is your final elastic net model? What were the alpha and lambda values? What is the prediction error?__

```{r}
Y <- data3$violentcrimes.perpop # extract Y
X <- model.matrix(violentcrimes.perpop~., data=data3)[, -1]
# now set alpha close to 1 so it does feature selection yet still benefits from Ridge Regression (comp. cheap, best in situations where least sq. estimates
# have high variance)
fit.elasnet.lambda <- glmnet(X, Y, alpha=.99) 
fit.elasnet.cv <- cv.glmnet(X, Y, alpha=.99, nfolds=10)  # using 10 folds to get best λ
plot(fit.elasnet.cv$lambda) #plot all λ values used
plot(log(fit.elasnet.cv$lambda), fit.elasnet.cv$cvm, xlab="log(lambda)", ylab="mean cv errors",pch=16, col="red") 
#cvm decreases till log(lambda)=~4.5 then sharply increases
plot(fit.elasnet.cv$lambda, fit.elasnet.cv$cvm, xlab="lambda", ylab="mean cv errors",
     pch=16, col="red")
#λ decreases initially, stays level until λ = 100 then begins increasing
# find minimizer of λ 
fit.elasnet.cv$lambda.min
fit.elasnet.cv$lambda.1se

set.seed(10)
# use appropriate minimizer of λ found via CV -- cv.glmnet 
lambda.manual <- 50
fit.final <- glmnet(X, Y, alpha=.99, lambda=lambda.manual)  # final elastic net fit
# names(fit.final)
beta.final <- coef(fit.final)
beta.final <- beta.final[which(beta.final !=0),]
beta.final <- as.matrix(beta.final)
rownames(beta.final)
#get variables in appropriate format to perform lm()
variables <- rownames(as.matrix(beta.final))
lm.input5 <- as.formula(paste("violentcrimes.perpop", "~", paste(variables[-1], collapse = "+"))) 
lm.input5
lm.final=lm(lm.input5,data3)
summary(lm.final)

```

#### Final elastic net model

```{r}
lm.input5
print("Prediction error:")
fit.elasnet.cv$cvm[lambda.manual]
print("Alpha = .99, lambda = 50")
```

__2. Use the elastic net variables in an OLS model. What is the equation, and what is the prediction error.__

```{r}
print("Equation:")
lm.input5
lm.final=lm(lm.input5,data3)
print("Prediction error: 364")
summary(lm.final)
```

__3. Summarize your findings, with particular focus on the difference between the two equations.__
 
```{r}
#coefs <- coef(fit.elasnet.cv, s=lambda.manual)
# coefs <- coef(fit.final, s=lambda.manual)
# coefs <- coefs[which(coef.min !=0),]
print("Elastic net equation:")
variables
# coefs
fit.min.lm2 <- lm(lm.input5, data=data3)
lm.output2 <- coef(fit.min.lm2) # output lm estimates
print("Linear model equation:")
lm.output2
summary(fit.min.lm2) 
# comp2 <- data.frame(coefs, lm.output2)
# names(comp2) <-c("estimates from LASSO", "lm estimates")
# comp2
```

As we can see the prediction error is much lower in the OLS model as opposed to the elastic net model.

**B+)** __Repeat similar stepts as that of **B)** but start with the set of variables that also include all two way interactions__

__1. How many variables do you have now?__

__2. Comparing the final models with the ones from **B)**, which one would you use? Commenting on your choice.__

