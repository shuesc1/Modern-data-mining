17 + 3
1 * 2 * 3 * 4 * 5 *
6 * 7 * 8 * 9 * 10
history()
c(0,1,1,2,3,5,8)
1:50
install.packages("MASS")
library("MASS")
help(package = "MASS")
vignette(package = "dplyr")
detach("package:MASS",unload=TRUE)
getwd
getwe()
getwd()
getwd()
d <- "/Users/josephhaymaker/Modern-data-mining"
d
setwd(d)
getwd()
radio <- read.csv("Survey_results_final.csv", header = TRUE, stringsAsFactors = F)
ff <- "https://cdn.rawgit.com/Keno/8573181/raw/7e97f56f521d1f49b966e04457687e87da1b062b/gistfile1.txt"
ff_example <- read.csv(curl::curl(ff), stringsAsFactors = F)
install.packages("curl")
ff_example <- read.csv(curl::curl(ff), stringsAsFactors = F)
head(ff_example, 10)
class(radio)
str(radio)
head(radio)
tail(radio)
ncol(radio)
names(radio)[1] <- "hit_id"
names(radio)[1:10]
tips <- read.csv("tips.txt", stringsAsFactors = T)
str(tips)
dim(tips)
head(tips)
head(tips, 10)
tail(tips, 10)
names(tips)
summary(tips)
tips$percent <- 100*tips$tip/tips$total_bill # create a new variable
str(tips)
plot(tips$total_bill,tips$percent)
plot(tips$total_bill,tips$percent,
main= "Total bill v. percent tip",
ylab = "Percent",
xlab = "Total bill",
pch = 16,
col = "red",
lwd = 2,
xlim = c(0,60),
ylim = c(0,50))
model <- lm(percent ~ total_bill -1, data = tips)
model
summary(model)
plot(tips$total_bill, tips$percent,
main = "Total Bill v. Percent Tip", # give plot a title
xlab = "Percent",    # label the x-axis
ylab = "Total Bill", # label the y-axis
pch = 16,            # change the type of plot point
col = "red",         # set the color of plot point
lwd = 2,             # set the line width
xlim = c(0,60),      # change limits of x-axis
ylim = c(0,50))      # change the limits of y-axis
abline(model) # add best fit line
getwd()
knitr::opts_chunk$set(echo = TRUE, eval = F)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, eval = F)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, eval = F)
install.packages("dplyr")
install.packages("ggplot2")
library(dplyr)
library(ggplot2)
datapay <- read.csv("MLPayData_Total.csv", header=T, as.is=F)
# You can also use the whole path to read in the data. In my case,
# datapay <- read.csv("/Users/lzhao/Dropbox/STAT471/Data/MLPayData_Total.csv", header=T, as.is=F)
getwd()
class(datapay)
str(datapay) # make sure the variables are correctly defined.
summary(datapay) # a quick summary of each variable
plot(datapay$payroll, datapay$avgwin,
pch  = 16,
cex  = 0.8,
col  = "blue",
xlab = "Payroll",
ylab = "Win Percentage",
main = "MLB Teams's Overall Win Percentage vs. Payroll")
text(datapay$payroll, datapay$avgwin, labels=datapay$team, cex=0.7, pos=1) # label all points
ggplot(datapay) +
geom_point(aes(x = payroll, y = avgwin), color = "blue") +
geom_text(aes(x = payroll, y = avgwin, label=team)) +
labs(title = "MLB Teams's Overall Win Percentage vs. Payroll", x = "Payroll", y = "Win Percentage")
datapay %>% select(team,payroll) %>% filter(payroll == max(payroll))
View(datapay)
head(datapay) # or tail(datapay)
head(datapay, 2) # first two rows
dim(datapay)
names(datapay)
datapay[1, 1]
datapay[1, "payroll"]
datapay$payroll # base R
datapay %>% select(payroll) # dplyr
datapay[, 1] # Base R
datapay %>% select(1) #DPLYR
datapay[, c(1:3)] # first three columns
datapay %>% select(1:3) #DPLYR
datapay %>% select(payroll:Team.name.2014) #DPLYR
datapay <- datapay %>% rename(team = Team.name.2014 ) # dplyr
names(datapay)[3] <- "team" # Base R
sum(is.na(datapay))
datapay %>%
filter(!complete.cases(.)) %>% summarize(n())
sapply(datapay, function(x) sum(is.na(x)))
data1 <- datapay[, -(21:37)] # take X1998 to X2014 out
data2 <- data1[, sort(names(data1))] # sort colnames
data2.1 <- data1[, order(names(data1))] # alternative way
names(data2)
names(data2.1)
data1 <- datapay %>% select(-21:-37)
data2 <- data1 %>% select(order(names(data1)))
names(data2)
myfit0 <- lm(avgwin ~ payroll, data=datapay)
names(myfit0)
summary(myfit0)   # it is another object that is often used
results <- summary(myfit0)
names(results)
plot(datapay$payroll, datapay$avgwin,
pch  = 16,
xlab = "Payroll",
ylab = "Win Percentage",
main = "MLB Teams's Overall Win Percentage vs. Payroll")
abline(myfit0, col="red", lwd=4)         # many other ways.
abline(h=mean(datapay$avgwin), lwd=5, col="blue") # add a horizontal line, y=mean(y)
ggplot(datapay, aes(x = payroll, y = avgwin)) +
geom_point() +
geom_smooth(method="lm", se = F,color = "red") +
geom_hline(aes(yintercept = mean(avgwin)), color = "blue") +
annotate(geom="text", x=0.8409340, y=0.5445067, label="Oakland A's",color="green", vjust = -1) +
annotate(geom="text", x=1.9723587, y=0.5487172, label="Red Sox",color="red", vjust = -1)
myfit0 <- lm(avgwin~payroll, data=datapay)
RSS <- sum((myfit0$res)^2) # residual sum of squares
RSS
sqrt(RSS/myfit0$df)
summary(myfit0)$sigma
knitr::opts_chunk$set(echo = TRUE)
head(est.boot$t, 20)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, rpart, pROC, partykit)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, rpart, pROC, partykit)
library(randomForest)
library(tree)
library(rpart)
library(pROC)
library(partykit)
library(rpart)
fit.tree <- rpart(HD~., data1, minsplit=1, cp=9e-3)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, rpart, pROC, partykit)
library(randomForest)
library(tree)
library(rpart)
library(pROC)
library(partykit)
library(rpart.plot)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, rpart, pROC, partykit)
library(randomForest)
library(tree)
library(rpart)
library(pROC)
library(partykit)
library(rattle)
install.packages('rattle')
library(rattle)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, rpart, pROC, partykit)
install.packages('rattle')
library(randomForest)
library(tree)
library(rpart)
library(pROC)
library(partykit)
library(rattle)
library('rattle')
install('rpart.plot')
install.packages('rpart.plot')
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, rpart, pROC, partykit)
#install.packages('rattle')
#install.packages('rpart.plot')
library(randomForest)
library(tree)
library(rpart)
library(pROC)
library(partykit)
# library(rattle)
#library(rpart.plot)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
if(!require('pacman')) {
install.packages('pacman')}
pacman::p_load( ISLR, tidyverse, tree, ramdomForest, ISLR, rpart, rattle, pROC, partykit, gbm)
options(repos = c(CRAN = "https://cran.rstudio.com"))
options(repos = c(CRAN = "https://cran.rstudio.com"))
randomForest::
library(randomForest)
library(ISLR)
help(Hitters)
data.comp <- na.omit(Hitters)  # For simplicity
dim(data.comp)            # We are keeping 263 players
data.comp$LogSalary <- log(data.comp$Salary)  # add LogSalary
names(data.comp)
data1 <- data.comp[,-19]
library(gbm) # boosting machine
data1 <- data1[sample(nrow(data1)), ] # make sure the row number plays no row!
ntree <- 200
fit.boost <- gbm(LogSalary ~., data = data1, distribution = "gaussian", n.trees = ntree, interaction.depth = 2,
train.fraction = .7)
names(fit.boost)
fit.boost$fit # hat y
fit.boost$train.error # training errors
fit.boost$valid.error # testing errors if train.fraction is given
yhat <- predict(fit.boost, newdata = data1, n.trees = ntree) # prediction, in this case it is the same as fit.boost$fit
ntree <- 20000
fit.boost <- gbm(LogSalary~., data = data1, distribution = "gaussian", n.trees = ntree, interaction.depth = 2,
train.fraction = .8)
gbm.perf(fit.boost, method ="test")
plot(fit.boost$valid.error, col= "red",
main="red: testing errors, black: training errors",
ylim = c(0, .9))
points(fit.boost$train.error, col="black")
n.t <- floor(.8*263)
data.train <- data1[1:n.t, ]   # n.t <- floor(.8*263)
data.test <- data1[-(1:n.t), ]
B <- gbm.perf(fit.boost, method ="test") # optimal number of trees
yhat <- predict(fit.boost, newdata = data.test, n.trees = gbm.perf(fit.boost, method ="test") )
fit.boost.test <- mean(yhat-data.test$LogSalary)^2  # the testing error for boosting
fit.rf.testing <- randomForest(LogSalary~., data.train, xtest=data.test[, -20],
ytest=data.test[,20], mtry=6, ntree=500)
fit.rf.testing$mse[500]   # testing error
fit.rf <-randomForest(LogSalary~., data.train,  mtry=6, ntree=500)
mean((predict(fit.rf, data.test)-data.test[, 20])^2)
# library(ISLR)
# gene.train <- data.frame(Khan$xtrain, tumor = Khan$ytrain)
# gene.test <- data.frame(Khan$xtest, tumor = Khan$ytest)
# gene <- rbind(gene.train, gene.test)
# genex <- rbind(data.frame(Khan$xtrain), data.frame(Khan$xtest))
# names(gene)
library(ISLR)
gene.train <- data.frame(Khan$xtrain, tumor = Khan$ytrain)
gene.test <- data.frame(Khan$xtest, tumor = Khan$ytest)
gene <- rbind(gene.train, gene.test)
genex <- rbind(data.frame(Khan$xtrain), data.frame(Khan$xtest))
names(gene)
library(dplyr)
library(data.table)
library(ggplot2)
gene <- data.table(gene)
tmp <- gene[, lapply(.SD, mean), by=tumor]
heatmap(genex)
gene.map <- data.frame(id = seq(1:nrow(gene)), genex)
gene.map.melt <- melt(gene.map, id="id")
gene.map.sub <- gene.map[1:10,]
gene.map.sub.melt <- melt(gene.map.sub, id="id")
ggplot(gene.map.sub.melt, aes(x=variable, y=id)) + geom_tile(aes(fill=value))
gene.map.sub <- gene[sample(nrow(gene), 15),]
gene.map.sub <- gene.map.sub[order(gene.map.sub$tumor),]
gene.map.sub <- data.frame(id = seq(1:nrow(gene.map.sub)), gene.map.sub)
gene.map.sub.melt <- melt(gene.map.sub, id="id")
ggplot(gene.map.sub.melt, aes(x=variable, y=id)) +
geom_tile(aes(fill=value)) +
ggtitle("from btm to top: 1-4")
data.frame(gene.map.sub$tumor, gene.map.sub$id)
names(gene, 30)
head(gene, 20)
head(gene, 10)
names(gene)
print(c(fit.boost.test,fit.rf.testing$mse[500] )) ### wowwww, who's a winner????
