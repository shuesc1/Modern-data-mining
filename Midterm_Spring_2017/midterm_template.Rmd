---
#============= SPRING 2017 =============
title: "Midterm Response - Spring 2017"
subtitle: "Modern Data Mining"
author: "Your Name"
date: "20 March 2017"
output: html_document
---

```{r setup, include=FALSE}
# To be safe, be at least R version 3.3.1 
knitr::opts_chunk$set(echo = TRUE, include = TRUE)
library(car)       # v 2.1-4
library(glmnet)    # v 2.0-5
library(dplyr)     # v 0.5.0
library(ggplot2)
library(GGally)
```

## Setup and Notes

We provide some glue code to load the data. 

```{r}
exam_data_url <- "https://cdn.rawgit.com/stillmatic/6d065a8d0963c229b1daa61277ff2b60/raw/e7462da3cd1eadf5219e2d26942fc37bb72fd3ef/exam_data_final.csv"
county_data <- read.csv(exam_data_url)

# data is available on canvas too, on the exam assignment page
# county_data <- read.csv("exam_data_final.csv")

# for windows:
# library(httr) # v 1.2.1
# county_data <- httr::content(httr::GET("exam_data_url"))
```

## for code chunk output

<!--- 
{r, include=FALSE, results='hide'}  -- don't include chunk in output, don't show results
{r, echo=FALSE, results='hide'} -- same
{r, echo=FALSE} -- don't show code, but show the output
{r, eval=FALSE} -- don't run the code at all
--->

Submit both this `Rmd` file and a knitted version (`html, pdf, or word`) to Canvas *before* 8 PM sharp!. The  submissions will be closed after 8:00 PM.

You *must* name this file according to the following scheme: `LastName_FirstName_Midterm.rmd`, e.g. `Zhao_Linda_Midterm.rmd`. 

## Part 1: Data Exploration 

### Question 1

Answer goes here

```{r}
## =============================  STANDARD EDA TECHNIQUES ==============================

## <<<< READING IN DATA >>>
# bill.data.test <- read.csv("Bills.subset.test.csv", header=TRUE, sep=",", na.strings="") # accounts for header, CSV, and na strings

#<<<<< SUMMARY STATS >>>>>>>>
dim(county_data)
#sum.isna(county_data)
# str(county_data)
summary(county_data)
tail(county_data, 10)
head(county_data, 20)
View(county_data)
#class(county_data)
names(county_data)
county_data[1,"adult_smoking"] # row 1 of column "adult_smoking"

# <<<<LEVELS of a categorical variable>>>
levels(county_data$state)
# <<<<<<setting the BASE LEVEL >>>>>>
# levels(county_data$median_income) # "HIGH", "LOW", "MEDIUM" (default is alphabetical order)
# county_data$median_income <- factor(county_data$median_income, levels = c("LOW", "MEDIUM", "HIGH"))

#<<<<< RENAME VARIABLES >>>>>>>>
#county_data <- county_data %>% rename(smoking_adults = adult_smoking ) # new = old name
names(county_data)

# <<<<<<<<<< NA VALUES >>>>>>>>>
sum(is.na(county_data))
# show how many NA values in each column
sapply(county_data, function(x) sum(is.na(x)))

# <<<<<< REMOVE A COLUMN >>>>>>>>
#remove a column
# county_data.mod <- county_data[,-1]
#head(county_data.mod)
#summary(county_data.mod)
#View(county_data.mod)

# <<<<<<<<<<<< GETTING DISTINCT VALUES IN A COLUMN >>>>>>>>>>>>
#county_data %>% county_data(,-1)
county_data %>% select(state) %>% distinct() # get distinct values for state column
county_data %>% select(median_income) %>% distinct() # get distince values for median_income column
#county_data %>% select(median_income) 
#base R way
#unique(county_data$state)

# <<<<<<< MIN/MAX VALUES FOR A COLUMN >>>>>>>>>
min(county_data$uninsured)
max(county_data$uninsured)
min(county_data$poverty_frac)
max(county_data$poverty_frac)

```

```{r}
# <<<<<<<<<<<<< PLOTS >>>>>>>>>>>>>>>>
#Histograms
hist(county_data$uninsured)
hist(county_data$poverty_frac)
hist(county_data$uninsured, breaks = 10)
hist(county_data$poverty_frac, breaks = 10)
# OR
ggplot(county_data) + geom_histogram(aes(x = poverty_frac), bins = 10, fill = "blue") +
  labs(title = "Histogram of the fraction of county residents in poverty", x = "Fraction in poverty", y = "Frequency")
ggplot(county_data) + geom_histogram(aes(x = uninsured), bins = 10, fill = "blue") +
  labs(title = "Histogram of the fraction of county residents uninsured", x = "Fraction uninsured", y = "Frequency")

# using 2 columns to get info --- states with highest and lowest median age
county_data$state[which.max(county_data$median_age)] # of all states which has highest median age
county_data$state[which.min(county_data$median_age)]
county_data$state[which.max(county_data$graduate_degree)]
county_data$county[which.max(county_data$median_age)]
county_data$county[which.min(county_data$median_age)]
county_data$county[which.max(county_data$graduate_degree)]
names(county_data)
county_data[county_data$dem12_frac > .75, "state"]

# boxplot -- poverty fraction by state
# boxplot(county_data$state, county_data$poverty_frac) #nope
county_data %>% ggplot(aes(x = state, y = poverty_frac)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(title = "Boxplots of fraction of county residents in poverty by state", x = "State", y = "Fraction in poverty")

# <<<<<< Scatter plot >>>>>>>>>>
ggplot(county_data) +
  geom_point(aes(x = poverty_frac, y = dem12_frac), color = "blue") +
  #geom_text(aes(x = poverty_frac, y = dem12_frac, label = state)) +
  labs(title = "State's poverty fraction and it's relationship to democrat voting fraction", x = "poverty fraction", y = "democratic vote percentage")

#<<< SCATTERPLOT WITH LS LINE ADDED >>>>>
plot.fit <- lm(dem12_frac~unemployment, data = county_data)
plot(county_data$unemployment, county_data$dem12_frac,
     pch = 16,
     xlab = "Percent unemployment",
     ylab = "Percent vote for Democrat candidate",
     main = "State's percent Democrat vote vs. unemployment")
abline(plot.fit, col="red", lwd=4)
abline(h=mean(county_data$dem12_frac), lwd=5, col="blue")

#<<<<< PAIRWISE SCATTER PLOT - correlation>>>>>>>
county_data %>% 
  select_if(is.numeric) %>%
  select(african_american_frac, latino_frac, caucasian_frac, dem12_frac, rep12_frac) %>%
  pairs()

```
Using EDA, we were able to ascertain that there are 11 states included in this dataset. These states are: 
California				
Maryland				
Michigan				
Pennsylvania				
Oklahoma				
Texas				
Wisconsin				
Georgia				
Kentucky				
Missouri

```{r}

summary(county_data$poverty_frac)
```

### Question 2
(plot of `uninsured` and `poverty_frac`, shown above)
### Question 3
(answered above)

## Part 2: Election Prediction

### Question 1

__1a. Answer goes here__

```{r}
# <<<< P VALUE >>>>>>
# Intuition for P value --> there is an x% chance that the true beta is zero (null hypothesis) --> since it's so small we are able to reject the null hypothesis
# ex: below -- there is a 0.00963 (.963% chance) that the true beta for median_incomeLow is zero
names(county_data)
summary(county_data)
summary(county_data$median_income)
lmfit0 <- lm(dem12_frac~median_income, data = county_data)
names(lmfit0)
lmfit0$coefficients
summary(lmfit0)
```
According to the summary statistics, when fitting a simple linear model predicting `dem12_frac` using only `median_income` we see that both `LOW` and `MEDIUM` median income levels are significant at the .01 level. 

__1b. Answer goes here__

According to the linear model discribed above, counties with a low median income are 2.7% _less_ likely to vote democrat with all else being equal. Similarly, counties with medium median incomes are 5.4% less likely to vote democratic when all else is equal. According to this model, high median income counties are most advantageous for democratic candidates, as both low and medium median income counties represent a decrease in percent of voters for the party. 

### Question 2

#### 2a)

```{r}
lmfit1 <- lm(dem12_frac~median_income + state, data = county_data)
summary(lmfit1)
Anova(lmfit1) # if each var in model is signicant holding all other variables the same
#anova(lmfit0, lmfit1)
summary(lmfit1)$coefficients
```
Yes, according to the F-test `median_income` is still significant in this new model at the 5% level, and we can reject $H_0$ that it isn't needed. 
As long as Fstat is above 2.14 we reject the null at .05 level.

#### 2B)

According to the above model if we hold state values equal, a low median income county will be associated with a corresponding 2.3% increase in percent population that votes democratic, while a medium median income county would be associated with a 1.7% drop in population that votes democratic.

This outcome differs from the first linear model in 1 very important way: this model implies that low median income counties are the most fruitful in terms of votes for the democratic party, while the first model implied that high median income communities would produce the most votes in favor of the party.



### Question 3

#### 3A)
```{r}
#install.packages('ISwR')
library(ISwR)
lmfit.full <- lm(dem12_frac~., data = county_data)
summary(lmfit.full)
Anova(lmfit.full)
# cor(county_data[name.num])

# <<<<<<<< CONFIDENCE INTERVAL >>>>>>>>
confint(lmfit1) # default 95% CI == beta +- 1.96 * SE(beta hat)
confint(lmfit.full)

# <<<<< CORRELATION OF VARIABLES >>>>>>>>>
county_data %>% 
  select_if(is.numeric) %>%
  select(dem12_frac, rep12_frac) %>%
  ggpairs()

# x <- county_data$dem12_frac
# y <- county_data$county
# cor(x, y)

#summary(lmfit.full)$coefficients

county_data %>% select(dem12_frac, rep12_frac) %>% cor()

```
#### 3B)

TODO: build a model using LASSO excluding `county` and `rep12_frac`, and `state`. 
Why use LASSO? Good if p (# parameters/variables) is really large -- forward stepwise selection an option, but backwards stepwise selection and exhaustive certainly are not. 
LASSO adds constraints to the coefficients -- L1 penalty, λ is tuning parameter. Best lambda is chosen by k-fold cross validation.
Use: `glmnet()`

```{r}
#<<<<<<<<<<< LASSO >>>>>>>>>>>>>>
set.seed(34)
names(county_data)
# extract y, dem12_frac, which is the 4th column
#Y <- county_data[,4]
Y <- county_data$dem12_frac
#Y
# county_data$dem12_frac
X <- model.matrix(dem12_frac~. -county -rep12_frac -state, data = county_data)[, -1]
#X
colnames(X)
fit.lasso.cv <- cv.glmnet(X, Y, alpha = 1, nfolds = 10)
fit.lasso.cv$lambda.1se
plot(fit.lasso.cv) # plots the possible values for lambda

# <<<<<<<< COEFFICIENTS FROM LASSO, WITH 10 FOLD CROSS VALIDATION USING LAMBDA.1SE >>>>>>>>
# can choose any value between lambda.min and lambda.1se
coef_1se <- coef(fit.lasso.cv, s = 'lambda.1se')
nzcoef <- rownames(coef_1se)[which((coef_1se) != 0)]
nzcoef
# -1 IS GETTING RID OF ALL THE CATEGORICAL VALUES
lm.input <- as.formula(paste("dem12_frac", "~", paste(nzcoef[-1], collapse = "+"))) 
# print <-  as.formula(paste(nzcoef[-1], collapse = "/n")) 
lm.input

# THIS WAY DIDN'T WORK TO GET THE COEFFICIENTS, but above did
# coef.min.1se <- coef(fit.lasso.cv, s="lambda.1se") #using 1se
# coef.min.1se <- coef.min.1se[which(coef.min.1se != 0)]
# coef.min.1se # set of predictors chosen
# rownames(as.matrix(coef.min.1se))
```
Non-zero coefficients specified by this model are:
dem12_frac ~ total_votes_12 + registered_voters + reg_vote_frac + 
    turnout + hs_diploma_degree + median_incomeLow + median_incomeMedium + 
    poverty_frac + caucasian_frac + gini_coefficient + median_age + 
    adult_smoking + uninsured + unemployment + violent_crime + 
    infant_mortality + latino_frac + bachelors_degree + graduate_degree + 
    margin_state_08

#### 3C)

```{r}
fit.ols.cv <- (lm(dem12_frac ~ total_votes_12 + registered_voters + reg_vote_frac + 
    turnout + hs_diploma_degree + median_income + 
    poverty_frac + caucasian_frac + gini_coefficient + median_age + 
    adult_smoking + uninsured + unemployment + violent_crime + 
    infant_mortality + latino_frac + bachelors_degree + graduate_degree + 
    margin_state_08, county_data))
summary(fit.ols.cv)
Anova(fit.ols.cv)
```
Not all variables are significant at the .05 level. In particular, the following 5 variables seem to contribute little to the model: `total_votes_12`, `median_age`, `registered_voters`
, `violent_crime`, and `margin_state_08`.


#### 3D) 

Manual backwards elimination
```{r}
fit.ols.cv <- (lm(dem12_frac ~ reg_vote_frac + turnout + 
    hs_diploma_degree + median_income + poverty_frac + 
    caucasian_frac + gini_coefficient + adult_smoking + 
    uninsured + unemployment + infant_mortality + 
    latino_frac + bachelors_degree + graduate_degree, county_data))
summary(fit.ols.cv)
Anova(fit.ols.cv)
```

```{r}
fit.ols.cv <- (lm(dem12_frac ~ reg_vote_frac + turnout + 
    hs_diploma_degree + poverty_frac + 
    caucasian_frac + 
    uninsured + unemployment +  
    latino_frac + bachelors_degree + graduate_degree, county_data))
summary(fit.ols.cv)
Anova(fit.ols.cv)

# CORRELATION BETWEEN VARIABLES still in the model
cor(county_data$latino_frac, county_data$caucasian_frac)
```

```{r}
# USING REGSUBSETS

Xy <- model.matrix(dem12_frac ~ median_income + total_votes_12 + registered_voters + reg_vote_frac + turnout + hs_diploma_degree + poverty_frac + caucasian_frac + gini_coefficient + median_age + adult_smoking + uninsured + unemployment + violent_crime + infant_mortality + latino_frac + graduate_degree + margin_state_08, county_data)[,-1]

Xy <- data.frame(Xy, county_data$dem12_frac)
b<-regsubsets(county_data.dem12_frac~.,Xy)
plot(summary(b)$cp)
```



#### 3e)  

Assumptions of a linear model: 
1. linearity - $E(y_i|x_i) = \beta_0 + \beta_1*x_i $
2. homoscedasticity - $var(y_i|x_i) = \sigma^2  $
3. normality - $y_i iid\ \mathcal{N}(\beta_0 + \beta_1 x_i \sigma^2)$

```{r}
#<<<<<<<<<< RESIDUAL PLOT -- LINEARITY & HOMOSCEDASTICITY >>>>>>>>>
plot(fit.ols.cv$fitted.values, fit.ols.cv$residuals,
     pch = 16,
     main = "residual plot")
abline(h=0, lwd=4, col="red")
```

Desirable residual plots are:
(1) they’re pretty symmetrically distributed, tending to cluster towards the middle of the plot
(2) they’re clustered around the lower single digits of the y-axis (e.g., 0.5 or 1.5, not 30 or 150)
(3) in general there aren’t clear patterns
These plots aren’t evenly distributed vertically, or they have an outlier, or they have a clear shape to them.
If you can detect a clear pattern or trend in your residuals, then your model has room for improvement.

So, what does random error look like for OLS regression? The residuals should not be either systematically high or low. So, the residuals should be centered on zero throughout the range of fitted values. In other words, the model is correct on average for all fitted values. Further, in the OLS context, random errors are assumed to produce residuals that are normally distributed. Therefore, the residuals should fall in a symmetrical pattern and have a constant spread throughout the range. Here's how residuals should look:


```{r}
#<<<<<<<< QQNORM PLOT FOR NORMALITY >>>>>>>>>
qqnorm(fit.ols.cv$residuals)
qqline(fit.ols.cv$residuals, lwd = 4, col="blue")
```

The deviations from the straight line are minimal. This indicates normal distribution.
The points clearly follow another shape than the straight line.
The histogram confirms the non-normality. The distribution is not bell-shaped but positively skewed (i.e., most data points are in the lower half). Histograms of normal distributions show the highest frequency in the center of the distribution.


#### 3D)

```{r}
summary(fit.ols.cv)
```

## Part 3 Turnout

### Question 1

```{r}
names(county_data)
lm.turnout <- lm(turnout~margin_state_08, data = county_data)
summary(lm.turnout)
Anova(lm.turnout)
```

#### 1A) 
no it doesn't.

#### 1B)

```{r}
lm.turnout2 <- lm(turnout~. -county -rep12_frac, data = county_data)
summary(lm.turnout2)
```

```{r}
pred <- data.frame(state = 'Washington',
                   county = 'King',
                   margin_state_08 = .5765 - .4048) # margin calculated from dem - republican vote
pred
```

```{r}
predict(lm.turnout, pred, interval = 'prediction', se = FALSE)
# turnout expected to be .45 - .85
# if you have a model with State in it then you won't be able to predict due to the fact that that WA isn't in original dataset
```
Example of creating a dummy entry for prediction:
```{r}
names(county_data)
test.county <- county_data[1,]
test.county
test.county$state <- "Idaho"
test.county$county <- "Butte"
# etc. for all vars
test.county
```



## ISLR: confidence interval vs. prediction interval

We use a _confidence interval_ to quantify the __uncertainty surrounding the average sales over a large number of cities (on the beta coefficient)__. For example, given that $100,000 is spent on TV advertising and $20,000 is spent on radio advertising in each city, the 95 % confidence interval is [10,985, 11,528]. We interpret this to mean that _95 % of intervals of this form will contain the true value of f(X).8_ On the other hand, a _prediction interval_ can be used to quantify the __uncertainty surrounding sales for a particular city__. Given that $100,000 is spent on TV advertising and $20,000 is spent on radio advertising in that city the 95 % prediction interval is [7,930, 14,580]. We interpret this to mean that _95 % of intervals of this form will contain the true value of Y for this city_. Note that both intervals are centered at 11,256, but that the prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty about sales for a given city in comparison to the average sales over many locations.


## Declaration

By submitting this document you certify that you have complied with the University of Pennsylvania's Code of Academic Integrity, to the best of your knowledge. You further certify that you have taken this exam under its sanctioned conditions, i.e. solely within the set exam room and within the time allotted. 